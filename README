L1 and L2 Regularization in Logistic Regression


MSc Machine Learning Tutorial
Author: Sonia Suvarana Dokala
Student ID: 24085938
Programme: MSc Data Science
University: University of Hertfordshire

ğŸ“š Overview

This tutorial investigates how L1 and L2 regularization improve the generalization of logistic regression models by controlling model complexity and reducing overfitting. Through clear explanations, formulas, and practical examples, it shows how each penalty affects weights, decision boundaries, and classification performance.

What Youâ€™ll Learn

How logistic regression works and how overfitting arises in highâ€‘capacity models.

The mathematical formulation of L1 (Lasso) and L2 (Ridge) penalties.

Geometric interpretation of â„“ 
1
  vs. â„“ 
2
  constraints in weight space.

When to choose L1, L2, or Elastic Net in real applications.

Practical steps for scaling features and tuning regularization strength using crossâ€‘validation.

ğŸš€ Quick Start

Installation

bash
# Clone the repository
git clone https://github.com/<your-username>/logistic-regularization-tutorial.git
cd logistic-regularization-tutorial

# Install dependencies
pip install -r requirements.txt

# Launch Jupyter Notebook
jupyter notebook notebooks/logistic_l1_l2_elasticnet.ipynb
Running the Tutorial

Open logistic_l1_l2_elasticnet.ipynb in Jupyter Notebook.

Run all cells sequentially (Cell â†’ Run All).

Plots, coefficient tables, and performance metrics for different penalties and Î» values will be generated automatically.

ğŸ“ Repository Structure

text
logistic-regularization-tutorial/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ logistic_l1_l2_elasticnet.ipynb   # Main Jupyter notebook
â”œâ”€â”€ L1-and-L2-Regularization-in-Logistic-Regression.pdf   # Written tutorial report
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ README.md                             # This file
â”œâ”€â”€ LICENSE                               # MIT License (example)
â””â”€â”€ .gitignore                            # Standard Python gitignore
ğŸ¯ Key Findings

The analysis in this tutorial highlights that:

L2 regularization (Ridge)

Produces smooth, proportional shrinkage of weights, keeping all features in the model with reduced magnitudes.

Offers strong stability, especially when features are correlated, and is suitable when all features may carry information.

L1 regularization (Lasso)

Drives many coefficients exactly to zero, performing automatic feature selection and yielding sparse, interpretable models.

Is effective when model simplicity and feature selection are priorities, or when many features are potentially irrelevant.

Elastic Net

Combines L1 and L2 to balance sparsity and stability, making it useful when features are correlated but interpretability still matters.

Hyperparameter Tuning

Proper scaling and crossâ€‘validated tuning of Î» are essential; too little regularization leads to overfitting, while too much causes underfitting and loss of predictive power.

ğŸ¨ Accessibility Features

This tutorial is designed with clarity and accessibility in mind:

Consistent headings and logical section structure for easy navigation.

Clear mathematical notation and stepâ€‘byâ€‘step explanations of key formulas.

Descriptive captions for plots illustrating decision boundaries and weight behaviour.

Wellâ€‘commented code cells in the notebook, separating data preparation, model training, and evaluation.

ğŸ“š References

Standard machine learning texts and tutorials on logistic regression and regularization (L1, L2, and Elastic Net).

Scikitâ€‘learn documentation for LogisticRegression and regularization parameters.

(Full academic references are provided in the accompanying PDF report.)

ğŸ‘¤ Author

Name: Sonia Suvarana Dokala
Student ID: 24085938
Programme: MSc Data Science
University: University of Hertfordshire

ğŸ“œ License

This project is (for example) licensed under the MIT License â€“ see the LICENSE file in the repository for full details.

ğŸ’¬ Contact and Assignment Details

For questions or feedback:

GitHub Issues: open an issue in the repository.

Email: yourâ€‘university email address (e.g. sd24ady@herts.ac.uk).

Assignment Details:

Module: Machine Learning / Neural Networks (or equivalent)



Weighting: 40% (example)



This README is structured for university submission and provides a clear entry point to the full report â€œL1 and L2 Regularization in Logistic Regression: A Comprehensive Guideâ€.
